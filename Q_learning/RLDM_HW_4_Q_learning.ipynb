{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #4\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this homework, you will have the complete reinforcement-learning experience:  training an agent from scratch to solve a simple domain using Q-learning.\n",
    "\n",
    "The environment you will be applying Q-learning to is called [Taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) (Taxi-v3).  The Taxi problem was introduced by [Dietterich 1998](https://www.jair.org/index.php/jair/article/download/10266/24463) and has been used for reinforcement-learning research in the past.  It is a grid-based environment where the goal of the agent is to pick up a passenger at one location and drop them off at another.\n",
    "\n",
    "The map is fixed and the environment has deterministic transitions.  However, the distinct pickup and drop-off points are chosen randomly from 4 fixed locations in the grid, each assigned a different letter.  The starting location of the taxicab is also chosen randomly.\n",
    "\n",
    "The agent has 6 actions: 4 for movement, 1 for pickup, and 1 for drop-off.  Attempting a pickup when there is no passenger at the location incurs a reward of -10.  Dropping off a passenger outside one of the four designated zones is prohibited, and attempting it also incurs a reward of −10.  Dropping the passenger off at the correct destination provides the agent with a reward of 20.  Otherwise, the agent incurs a reward of −1 per time step.\n",
    "\n",
    "Your job is to train your agent until it converges to the optimal state-action value function.  You will have to think carefully about algorithm implementation, especially exploration parameters.\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a fundamental reinforcement-learning algorithm that has been successfully used to solve a variety of  decision-making  problems.   Like  Sarsa,  it  is  a  model-free  method  based  on  temporal-difference  learning. However, unlike Sarsa, Q-learning is *off-policy*, which means the policy it learns about can be different than the policy it uses to generate its behavior.  In Q-learning, this *target* policy is the greedy policy with respect to the current value-function estimate.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "- You should return the optimal *Q-value* for a specific state-action pair of the Taxi environment.\n",
    "\n",
    "- To solve this problem you should implement the Q-learning algorithm and use it to solve the Taxi environment. The agent  should  explore  the MDP, collect data  to  learn an optimal  policy and also the optimal Q-value function.  Be mindful of how you handle terminal states: if $S_t$ is a terminal state, then $V(St)$ should always be 0.  Use $\\gamma= 0.90$ - this is important, as the optimal value function depends on the discount rate.  Also, note that an $\\epsilon$-greedy strategy can find an optimal policy despite finding sub-optimal Q-values.   As we are looking for optimal  Q-values, you will have to carefully consider your exploration strategy.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Lesson 7: Exploring Exploration\n",
    "\n",
    "-   Chapter 6 (6.5 Q-learning: Off-policy TD Control) of http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "-   Chapter 2 (2.6.1 Q-learning) of 'Algorithms for Sequential Decision Making', M.\n",
    "    Littman, 1996\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have set your timezone in Canvas to ensure the deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope. \n",
    "    Be cognisant of performance.  If the autograder fails because of memory \n",
    "    or runtime issues, you need to refactor your solution\n",
    "\n",
    "-   Gradescope is using *python 3.6.x*, *gym==0.17.2* and *numpy==1.18.0*, and you can\n",
    "    use any core library (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# gym==0.17.2\n",
    "# numpy==1.18.0\n",
    "################\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "class QLearningAgent(object):\n",
    "    def __init__(self):\n",
    "        self.whoami = '903658755'\n",
    "        self.Q = None\n",
    "        self.alpha = 0.9\n",
    "        self.gamma = 0.9\n",
    "        # higher the epsilon higher will be the exploration and lesser will be the exploitation\n",
    "        self.epsilon = 0.2\n",
    "        self.env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "    def action_epsilon_greedy(self, current_state):\n",
    "        return self.env.action_space.sample() if np.random.uniform(0, 1) < self.epsilon else np.argmax(\n",
    "            self.Q[current_state, :])\n",
    "\n",
    "    def calculate_td_update(self, reward, state, action, next_state):\n",
    "        return reward + self.gamma * np.max(self.Q[next_state, :]) - self.Q[state, action]\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"Create the Q table\"\"\"\n",
    "        n_episodes = 100000\n",
    "        no_states = self.env.observation_space.n\n",
    "        no_actions = self.env.action_space.n\n",
    "        self.Q = np.zeros((no_states, no_actions))\n",
    "        for episode_no in range(n_episodes):\n",
    "            current_state = self.env.reset()\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                current_action = self.action_epsilon_greedy(current_state)\n",
    "                new_state, reward, finished, information = self.env.step(current_action)\n",
    "                self.Q[current_state, current_action] += self.alpha * self.calculate_td_update(reward, current_state,\n",
    "                                                                                               current_action,\n",
    "                                                                                               new_state)\n",
    "                current_state = new_state\n",
    "        self.env.close()\n",
    "\n",
    "    def Q_table(self, state, action):\n",
    "        return self.Q[state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that you submission is correct \n",
    "## and will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.agent = QLearningAgent()\n",
    "        cls.agent.solve()\n",
    "        \n",
    "    def test_case_1(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(462, 4),\n",
    "            -11.374402515,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_2(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(398, 3),\n",
    "            4.348907,\n",
    "            decimal=3\n",
    "        )\n",
    "    \n",
    "    def test_case_3(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(253, 0),\n",
    "            -0.5856821173,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_4(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(377, 1),\n",
    "            9.683,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_5(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(83, 5),\n",
    "            -13.9968,\n",
    "#            -12.8232,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
