{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #3\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "For this assignment,  you will build a Sarsa agent which will learn policies in the [OpenAI Gym](http://gym.openai.com/docs/) Frozen Lake environment.  [OpenAI Gym](http://gym.openai.com/docs/) is a platform where users can test their RL algorithms on a selection of carefully crafted environments.  As we will continue to use [OpenAI Gym](http://gym.openai.com/docs/) through Project 2, this assignment also provides an opportunity to familiarize yourself with its interface.\n",
    "\n",
    "Frozen Lake is a grid world environment that is highly stochastic,  where the agent must cross a slippery frozen  lake  which  has  deadly  holes  to  fall  through.   The  agent  begins  in  the  starting  state `S` and  is  given  a reward of `1` if it reaches the goal state `G`.  A reward of `0` is given for all other transitions.\n",
    "\n",
    "The agent can take one of four possible moves at each state (left, down, right, or up).  The frozen cells `F` are slippery, so the agent’s actions succeed only `1/3` of the time, while the other `2/3` are split evenly between the two directions orthogonal to the intended direction.  If the agent lands in a hole `H`, then the episode terminates. You will be given a randomized Frozen Lake map with a corresponding set of parameters to train your Sarsa agent with.  If your agent is implemented correctly, then after training it for the specified number of episodes, your agent will produce the same policy (not necessarily an optimal policy) as the automatic grader.\n",
    "\n",
    "\n",
    "## Sarsa $($$S_t$, $A_t$, $R_{t+1}$, $S_{t+1}$, $A_{t+1}$$)$\n",
    "\n",
    "Sarsa uses temporal-difference learning to form a model-free on-policy reinforcement-learning algorithm that solves the *control* problem. It is model free because it does not need and does not use a model of the environment, namely neither a transition nor reward function; instead, Sarsa samples transitions and rewards online.\n",
    "\n",
    "It is on-policy because it learns about the same policy that generates its behaviors (this is in contrast to *Q-learning*, which you’ll examine in your next homework).  That is, Sarsa estimates the action-value function of its behavior policy.  In this homework,  you will not be training a Sarsa agent to approximate the *optimal* action-value function; instead, the hyperparameters of both the exploration strategy and the algorithm will be given to you as input — the goal being to verify that your SARSA agent is correctly implemented.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "Since this homework requires you to match a non-deterministic output between your agent and the autograder’s agent, attention to detail to each of the following points is required:\n",
    "\n",
    "- You must use Python and the library NumPy for this homework *python 3.6.x* and *numpy==1.18.0*\n",
    "\n",
    "- Install OpenAI Gym (e.g.pip install gym) *gym==0.17.2*\n",
    "\n",
    "- Instantiate the Frozen Lake environment with **gym.make('FrozenLake-v0', desc=my_desc).unwrapped**,  where the attribute desc is a square version of the string input **amap**.  This means you must resize amap so that it looks like a square grid, where the input characters fill in the grid row by row.  Make sure you use the unwrapped method as indicated above.\n",
    "\n",
    "- The input **seed** should be used to seed the random number generators for both Gym and NumPy.  Do *not* use the Python standard library’s *random* library.\n",
    "\n",
    "- Implement your Sarsa agent using an $\\epsilon$-greedy behavioral policy.  Specifically, you must use *numpy.random.random* to  choose  whether  or  not  the  action  is  greedy,  and *numpy.random.randint* to select the random action.\n",
    "\n",
    "- Initialize the agent’s Q-table to zeros.\n",
    "\n",
    "- Train your agent using the given input parameters.  The input *amap* is the Frozen Lake map that you need to resize and provide to the *desc* attribute when you instantiate your environment.  The input *gamma* is the discount rate.  The input *alpha* is the learning rate.  The input *epsilon* is the parameter for the $\\epsilon$-greedy behavior strategy your Sarsa agent will use.  Specifically, an action should be selected uniformly at random if a random number drawn uniformly between 0 and 1 is less than $\\epsilon$.  If the greedy action is selected,  the  action  with  lowest  index  should  be  selected  in  case  of  ties.   The  input `n_episodes` is  the number of episodes to train your agent.  Finally, *seed* is the number used to seed both Gym’s random number generator and NumPy’s random number generator.\n",
    "\n",
    "- To sync with the autograder,  your Sarsa implementation should select the action corresponding to the next state the  agent  will  visit *even when* that  next  state  is  a  terminal  state  (this  action  will  never  be executed by the agent).\n",
    "\n",
    "- You should return the greedy policy with respect to the Q-function obtained by your Sarsa agent after the completion of the final episode.  Specifically, the policy should be expressed as a string ofcharacters: **<, v, >, ^,** representing left, down, right, and up, respectively.  The ordering of the actions in the output should reflect the ordering of states in *amap*. \n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Chapter 6 (6.4 Sarsa:  On-policy TD Control) of\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have your timezone in Canvas set to ensure the\n",
    "    deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope. \n",
    "    Be cognisant of performance.  If the autograder fails because of memory \n",
    "    or runtime issues, you need to refactor your solution\n",
    "\n",
    "-   Gradescope is using *python 3.6.x*, *gym==0.17.2* and *numpy==1.18.0*, and you can\n",
    "    use any core library (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# numpy==1.18.0\n",
    "# gym==0.17.2\n",
    "################\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from gym.envs import toy_text\n",
    "\n",
    "class FrozenLakeAgent(object):\n",
    "    def __init__(self):\n",
    "        self.whoami = '903658755'\n",
    "\n",
    "    def amap_to_gym(self, amap='FFGG'):\n",
    "        \"\"\"Maps the `amap` string to a gym env\"\"\"\n",
    "        amap = np.asarray(amap, dtype='c')\n",
    "        side = int(sqrt(amap.shape[0]))\n",
    "        amap = amap.reshape((side, side))\n",
    "        return gym.make('FrozenLake-v0', desc=amap).unwrapped\n",
    "\n",
    "    def action_epsilon_greedy(self, q_table, epsilon, current_state):\n",
    "        return np.random.randint(0, 4) if np.random.uniform(0, 1) < epsilon else np.argmax(q_table[current_state, :])\n",
    "\n",
    "    def calculate_td_update(self, q_table, reward, gamma, state, action, next_state, next_action):\n",
    "        return reward + gamma * q_table[next_state, next_action] - q_table[state, action]\n",
    "\n",
    "    def read_out_policy(self, q_table, action_dict):\n",
    "        return [action_dict[np.argmax(q_table[i, :])] for i in range(q_table.shape[0])]\n",
    "\n",
    "    def solve(self, amap, gamma, alpha, epsilon, n_episodes, seed):\n",
    "        \"\"\"Implement the agent\"\"\"\n",
    "        env = self.amap_to_gym(amap)\n",
    "        np.random.seed(seed)\n",
    "        env.seed(seed)\n",
    "        action_dict = {0: '<', 1: 'v', 2: '>', 3: '^'}\n",
    "        no_states = env.observation_space.n\n",
    "        no_actions = env.action_space.n\n",
    "        q_table = np.zeros((no_states, no_actions))\n",
    "        for episode_no in range(n_episodes):\n",
    "            current_state = env.reset()\n",
    "            current_action = self.action_epsilon_greedy(q_table, epsilon, current_state)\n",
    "            finished = False\n",
    "            while not finished:\n",
    "                new_state, reward, finished, information = env.step(current_action)\n",
    "                new_action = self.action_epsilon_greedy(q_table, epsilon, new_state)\n",
    "                q_table[current_state, current_action] += alpha * self.calculate_td_update(q_table, reward, gamma,\n",
    "                                                                                           current_state,\n",
    "                                                                                           current_action, new_state,\n",
    "                                                                                           new_action)\n",
    "                current_state = new_state\n",
    "                current_action = new_action\n",
    "        env.close()\n",
    "        policy_list = self.read_out_policy(q_table, action_dict)\n",
    "        return ''.join([str(entry) for entry in policy_list])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that you submission is correct \n",
    "## and will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.agent = FrozenLakeAgent()\n",
    "\n",
    "    def test_case_1(self):\n",
    "        example1 = self.agent.solve(\n",
    "            amap='SFFFHFFFFFFFFFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.25,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=14697,\n",
    "            seed=741684\n",
    "        )\n",
    "        assert(example1 == '^vv><>>vvv>v>>><')\n",
    "\n",
    "    def test_case_2(self):\n",
    "        example2 = self.agent.solve(\n",
    "            amap='SFFFFHFFFFFFFFFFFFFFFFFFG',\n",
    "            gamma=0.91,\n",
    "            alpha=0.12,\n",
    "            epsilon=0.13,\n",
    "            n_episodes=42271,\n",
    "            seed=983459\n",
    "        )\n",
    "        assert(example2 == '^>>>><>>>vvv>>vv>>>>v>>^<')\n",
    "\n",
    "    def test_case_3(self):\n",
    "        example3 = self.agent.solve(\n",
    "            amap='SFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.24,\n",
    "            epsilon=0.09,\n",
    "            n_episodes=49553,\n",
    "            seed=20240\n",
    "        )\n",
    "        assert(example3 == '<<v<')\n",
    "\n",
    "    def test_case_4(self):\n",
    "        example4 = self.agent.solve(\n",
    "            amap='SFFHHFFHHFFHHFFG',\n",
    "            gamma=0.99,\n",
    "            alpha=0.5,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=23111,\n",
    "            seed=44323\n",
    "        )\n",
    "        assert(example4=='^><<<>^<<><<<>^<')\n",
    "\n",
    "    def test_case_5(self):\n",
    "        example5 = self.agent.solve(\n",
    "            amap='SFFFFHFFFHHFFFFFFFFHHFFFG',\n",
    "            gamma=0.88,\n",
    "            alpha=0.15,\n",
    "            epsilon=0.16,\n",
    "            n_episodes=112312,\n",
    "            seed=6854343\n",
    "        )\n",
    "        assert(example5 == '^>><^<>><<<>v<^v>v<<<>vv<')\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
