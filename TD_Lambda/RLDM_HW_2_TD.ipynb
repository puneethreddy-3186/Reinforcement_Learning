{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #2\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The $\\lambda$ Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "Given an MDP and a particular time step $t$ of a task (continuing or\n",
    "episodic), the $\\lambda$-return, $G_t^\\lambda$, $0\\leq\\lambda\\leq 1$, is\n",
    "a weighted combination of the $n$-step returns $G_{t:t+n}$, $n \\geq 1$:\n",
    "\n",
    "$${G_t^\\lambda = \\sum\\limits_{n=1}^\\infty(1-\\lambda)\\lambda^{n-1}G_{t:t+n}.}$$\n",
    "\n",
    "While the $n$-step return $G_{t:t+n}$ can be viewed as the target of\n",
    "an $n$-step TD update rule, the $\\lambda$-return can be viewed as the\n",
    "target of the update rule for the TD$(\\lambda)$ prediction algorithm,\n",
    "which you will become familiar with in project 1.\n",
    "\n",
    "Consider the Markov reward process described by the following state\n",
    "diagram and assume the agent is in state $0$ at time $t$ (also assume\n",
    "the discount rate is $\\gamma=1$). A Markov reward process can be thought of as\n",
    "an MDP with only one action possible from each state (denoted as action $0$ in\n",
    "the figure below).\n",
    "\n",
    "![image](https://d1b10bmlvqabco.cloudfront.net/paste/jqmfo7d3watba/9e6fd83672f880704b8418728297fc077786c8907d87fec631601da9ff4c85ef/hw2.png)\n",
    "\n",
    "## Procedure\n",
    "\n",
    "-   You will implement your solution using the `solve()` method\n",
    "    in the code below.\n",
    "    \n",
    "-   You will be given `p`, the probability of transitioning from state\n",
    "    $0$ to state $1$, `V`, the estimate of the value function at time\n",
    "    $t$, represented as a vector\n",
    "    $[V(0), V(1), V(2), V(3), V(4), V(5), V(6)]$, and `rewards`, a\n",
    "    vector of the rewards $[r_0, r_1, r_2, r_3, r_4, r_5, r_6]$\n",
    "    corresponding to the MDP.\n",
    "\n",
    "-   Your return value should be a value of $\\lambda$,\n",
    "    strictly less than 1, such that the expected value of the\n",
    "    $\\lambda$-return equals the expected Monte-Carlo return at time $t$.\n",
    "\n",
    "-   Your answer must be correct to $3$ decimal places, truncated (e.g.\n",
    "    3.14159265 becomes 3.141).\n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lecture Lesson 3: TD and Friends\n",
    "\n",
    "-   Chapter 7 (7.1 $n$-step TD Prediction) and Chapter 12 (12.1 The\n",
    "    $\\lambda$-return) of\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "-   'Learning to Predict by the Method of Temporal Differences', R.\n",
    "    Sutton, 1988\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have your timezone in Canvas set to ensure the\n",
    "    deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope.\n",
    "\n",
    "-   Gradescope is using *python 3.6.x* and *numpy==1.18.0*, and you can\n",
    "    use any core library (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# numpy==1.18.0\n",
    "################\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TDAgent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_inputs(self, p, V, rewards):\n",
    "        are_inputs_valid = True\n",
    "        if p < 0 or p > 1:\n",
    "            are_inputs_valid = False\n",
    "        if len(V) == 0:\n",
    "            are_inputs_valid = False\n",
    "        if len(rewards) == 0:\n",
    "            are_inputs_valid = False\n",
    "        return are_inputs_valid\n",
    "\n",
    "    def fetch_rewards(self, rewards, sample_seq):\n",
    "        if sample_seq == 1:\n",
    "            episode = [rewards[0], rewards[2], rewards[4], rewards[5], rewards[6]]\n",
    "        else:\n",
    "            episode = [rewards[1], rewards[3], rewards[4], rewards[5], rewards[6]]\n",
    "        return episode\n",
    "\n",
    "    def fetch_value_funcs(self, V, sample_seq):\n",
    "        if sample_seq == 1:\n",
    "            episode = [V[0], V[1], V[3], V[4], V[5], V[6]]\n",
    "        else:\n",
    "            episode = [V[0], V[2], V[3], V[4], V[5], V[6]]\n",
    "        return episode\n",
    "\n",
    "    def calculate_expected_mc_return(self, p, rewards):\n",
    "        return p * sum(self.fetch_rewards(rewards, 1)) + (1 - p) * sum(self.fetch_rewards(rewards, 2))\n",
    "\n",
    "    def fetch_n_step_td_update(self, V, rewards, sample_seq, no_steps):\n",
    "        state_value_funcs = self.fetch_value_funcs(V, sample_seq)\n",
    "        episode_rewards = self.fetch_rewards(rewards, sample_seq)\n",
    "        n_step_rewards = [episode_rewards[i] for i in range(no_steps)]\n",
    "        value = sum(n_step_rewards) + state_value_funcs[no_steps]\n",
    "        return value\n",
    "\n",
    "    def calculate_n_step_td_return(self, p, V, rewards, no_steps):\n",
    "        first_term = p * self.fetch_n_step_td_update(V, rewards, 1, no_steps)\n",
    "        second_term = (1 - p) * self.fetch_n_step_td_update(V, rewards, 2, no_steps)\n",
    "        return first_term + second_term\n",
    "\n",
    "    def solve(self, p, V, rewards):\n",
    "        if self.validate_inputs(p, V, rewards):\n",
    "            n_step_returns = list()\n",
    "            for i in range(1, 6):\n",
    "                n_step_returns.append(self.calculate_n_step_td_return(p, V, rewards, i))\n",
    "            expected_mc_return = self.calculate_expected_mc_return(p, rewards)\n",
    "            n_step_returns.append(0)\n",
    "            a = np.array(n_step_returns)\n",
    "            n_step_returns1 = list()\n",
    "            n_step_returns1.append(0)\n",
    "            n_step_returns1.extend(n_step_returns[0:len(n_step_returns) - 1])\n",
    "            b = np.array(n_step_returns1) * -1\n",
    "            n_step_returns2 = list()\n",
    "            n_step_returns2.extend([-expected_mc_return, 0, 0, 0, 0, n_step_returns[-2]])\n",
    "            c = np.array(n_step_returns2)\n",
    "            array_tuple = (a, b, c)\n",
    "            arrays = np.vstack(array_tuple)\n",
    "            coeff = arrays.sum(axis=0)\n",
    "            roots = np.roots(coeff[::-1])\n",
    "            real_roots = roots[np.isreal(roots)]\n",
    "            real_parts = [np.real(rt) for rt in real_roots]\n",
    "            real_positive_roots = [rt for rt in real_parts if rt > 0 and rt < 1]\n",
    "            final_root_array = [rt1 for rt1 in real_positive_roots if abs(1 - rt1) > 1e-3]\n",
    "            final_root = math.trunc(final_root_array[0] * 1000) * .001\n",
    "            return final_root\n",
    "        return 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_case_1 (__main__.TestTDNotebook) ... ok\n",
      "test_case_2 (__main__.TestTDNotebook) ... ok\n",
      "test_case_3 (__main__.TestTDNotebook) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x28f794d6970>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################\n",
    "# DO NOT REMOVE\n",
    "# Versions\n",
    "# numpy==1.18.0\n",
    "################\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class TDAgent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def validate_inputs(self, p, V, rewards):\n",
    "        are_inputs_valid = True\n",
    "        if p < 0 or p > 1:\n",
    "            are_inputs_valid = False\n",
    "        if len(V) == 0:\n",
    "            are_inputs_valid = False\n",
    "        if len(rewards) == 0:\n",
    "            are_inputs_valid = False\n",
    "        return are_inputs_valid\n",
    "\n",
    "    def fetch_rewards(self, rewards, sample_seq):\n",
    "        if sample_seq == 1:\n",
    "            episode = [rewards[0], rewards[2], rewards[4], rewards[5], rewards[6]]\n",
    "        else:\n",
    "            episode = [rewards[1], rewards[3], rewards[4], rewards[5], rewards[6]]\n",
    "        return episode\n",
    "\n",
    "    def fetch_value_funcs(self, V, sample_seq):\n",
    "        if sample_seq == 1:\n",
    "            episode = [V[0], V[1], V[3], V[4], V[5], V[6]]\n",
    "        else:\n",
    "            episode = [V[0], V[2], V[3], V[4], V[5], V[6]]\n",
    "        return episode\n",
    "\n",
    "    def calculate_expected_mc_return(self, p, rewards):\n",
    "        return p * sum(self.fetch_rewards(rewards, 1)) + (1 - p) * sum(self.fetch_rewards(rewards, 2))\n",
    "\n",
    "    def fetch_n_step_td_update(self, V, rewards, sample_seq, no_steps):\n",
    "        state_value_funcs = self.fetch_value_funcs(V, sample_seq)\n",
    "        episode_rewards = self.fetch_rewards(rewards, sample_seq)\n",
    "        n_step_rewards = [episode_rewards[i] for i in range(no_steps)]\n",
    "        value = sum(n_step_rewards) + state_value_funcs[no_steps]\n",
    "        return value\n",
    "\n",
    "    def calculate_n_step_td_return(self, p, V, rewards, no_steps):\n",
    "        first_term = p * self.fetch_n_step_td_update(V, rewards, 1, no_steps)\n",
    "        second_term = (1 - p) * self.fetch_n_step_td_update(V, rewards, 2, no_steps)\n",
    "        return first_term + second_term\n",
    "\n",
    "    def solve(self, p, V, rewards):\n",
    "        if self.validate_inputs(p, V, rewards):\n",
    "            n_step_returns = list()\n",
    "            for i in range(1, 6):\n",
    "                n_step_returns.append(self.calculate_n_step_td_return(p, V, rewards, i))\n",
    "            expected_mc_return = self.calculate_expected_mc_return(p, rewards)\n",
    "            n_step_returns.append(0)\n",
    "            a = np.array(n_step_returns)\n",
    "            n_step_returns1 = list()\n",
    "            n_step_returns1.append(0)\n",
    "            n_step_returns1.extend(n_step_returns[0:len(n_step_returns) - 1])\n",
    "            b = np.array(n_step_returns1) * -1\n",
    "            n_step_returns2 = list()\n",
    "            n_step_returns2.extend([-expected_mc_return, 0, 0, 0, 0, n_step_returns[-2]])\n",
    "            c = np.array(n_step_returns2)\n",
    "            array_tuple = (a, b, c)\n",
    "            arrays = np.vstack(array_tuple)\n",
    "            coeff = arrays.sum(axis=0)\n",
    "            roots = np.roots(coeff[::-1])\n",
    "            real_roots = roots[np.isreal(roots)]\n",
    "            real_parts = [np.real(rt) for rt in real_roots]\n",
    "            real_positive_roots = [rt for rt in real_parts if rt > 0 and rt < 1]\n",
    "            final_root_array = [rt1 for rt1 in real_positive_roots if abs(1 - rt1) > 1e-3]\n",
    "            final_root = math.trunc(final_root_array[0] * 1000) * .001\n",
    "            return final_root\n",
    "        return 0.000\n",
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that your submission\n",
    "## will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestTDNotebook(unittest.TestCase):\n",
    "    def test_case_1(self):\n",
    "        agent = TDAgent()\n",
    "        np.testing.assert_almost_equal(\n",
    "            agent.solve(\n",
    "                p=0.81,\n",
    "                V=[0.0, 4.0, 25.7, 0.0, 20.1, 12.2, 0.0],\n",
    "                rewards=[7.9, -5.1, 2.5, -7.2, 9.0, 0.0, 1.6]\n",
    "            ),\n",
    "            0.622,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_2(self):\n",
    "        agent = TDAgent()\n",
    "        np.testing.assert_almost_equal(\n",
    "            agent.solve(\n",
    "                p=0.22,\n",
    "                V=[12.3, -5.2, 0.0, 25.4, 10.6, 9.2, 0.0],\n",
    "                rewards=[-2.4, 0.8, 4.0, 2.5, 8.6, -6.4, 6.1]\n",
    "            ),\n",
    "            0.519,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_3(self):\n",
    "        agent = TDAgent()\n",
    "\n",
    "        np.testing.assert_almost_equal(\n",
    "            agent.solve(\n",
    "                p=0.64,\n",
    "                V=[-6.5, 4.9, 7.8, -2.3, 25.5, -10.2, 0.0],\n",
    "                rewards=[-2.4, 9.6, -7.8, 0.1, 3.4, -2.1, 7.9]\n",
    "            ),\n",
    "            0.207,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases\n",
    "\n",
    "We have provided some test cases for you to help verify your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
